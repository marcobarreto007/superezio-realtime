# -*- coding: utf-8 -*-
"""
Testes b√°sicos para configura√ß√£o do backend Python
Testes que N√ÉO carregam o modelo (r√°pidos)
"""
import sys
import os
from pathlib import Path

# Adicionar backend ao path
BACKEND_DIR = Path(__file__).parent
sys.path.insert(0, str(BACKEND_DIR))

def test_imports():
    """Testa se todos os imports necess√°rios funcionam"""
    try:
        import torch
        import transformers
        import fastapi
        import uvicorn
        from peft import PeftModel
        print("‚úÖ Todos os imports funcionando")
        return True
    except ImportError as e:
        print(f"‚ùå Erro ao importar: {e}")
        return False

def test_torch_cuda():
    """Testa se CUDA est√° dispon√≠vel"""
    import torch
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        device_name = torch.cuda.get_device_name(0)
        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"‚úÖ CUDA dispon√≠vel: {device_name} ({total_memory:.1f} GB)")
    else:
        print("‚ö†Ô∏è  CUDA n√£o dispon√≠vel - rodar√° em CPU")
    return True

def test_model_path():
    """Testa se o caminho do modelo est√° correto"""
    from inference import LOCAL_MODEL_DIR, LORA_ADAPTER_DIR

    if LOCAL_MODEL_DIR.exists():
        print(f"‚úÖ Modelo encontrado: {LOCAL_MODEL_DIR}")
        # Verificar arquivos cr√≠ticos
        config_file = LOCAL_MODEL_DIR / "config.json"
        if config_file.exists():
            print(f"‚úÖ config.json encontrado")
        else:
            print(f"‚ö†Ô∏è  config.json N√ÉO encontrado")
    else:
        print(f"‚ùå Modelo N√ÉO encontrado: {LOCAL_MODEL_DIR}")
        return False

    # LoRA √© opcional
    if LORA_ADAPTER_DIR.exists():
        print(f"‚úÖ Adaptador LoRA encontrado: {LORA_ADAPTER_DIR}")
    else:
        print(f"‚ÑπÔ∏è  Adaptador LoRA n√£o encontrado (opcional)")

    return True

def test_api_endpoints():
    """Testa se os endpoints da API est√£o definidos"""
    try:
        from api import app
        routes = [route.path for route in app.routes]

        expected_routes = ["/", "/health", "/chat", "/chat/stream"]
        for route in expected_routes:
            if route in routes:
                print(f"‚úÖ Endpoint '{route}' definido")
            else:
                print(f"‚ùå Endpoint '{route}' N√ÉO encontrado")
                return False

        return True
    except Exception as e:
        print(f"‚ùå Erro ao verificar endpoints: {e}")
        return False

def run_all_tests():
    """Executa todos os testes"""
    print("="*60)
    print("üß™ TESTES DE CONFIGURA√á√ÉO BACKEND")
    print("="*60)

    tests = [
        ("Imports", test_imports),
        ("CUDA/GPU", test_torch_cuda),
        ("Caminho do Modelo", test_model_path),
        ("Endpoints da API", test_api_endpoints),
    ]

    results = []
    for name, test_func in tests:
        print(f"\nüìã Teste: {name}")
        print("-"*60)
        try:
            result = test_func()
            results.append((name, result))
        except Exception as e:
            print(f"‚ùå Exce√ß√£o no teste '{name}': {e}")
            import traceback
            traceback.print_exc()
            results.append((name, False))

    # Resumo
    print("\n" + "="*60)
    print("üìä RESUMO DOS TESTES")
    print("="*60)
    passed = sum(1 for _, result in results if result)
    total = len(results)

    for name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status} - {name}")

    print(f"\nTotal: {passed}/{total} testes passaram")
    print("="*60)

    return passed == total

if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
