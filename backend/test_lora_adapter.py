"""
Teste Isolado - Verificar se o Adaptador LoRA est√° sendo usado
Testa o modelo base vs modelo com LoRA para confirmar diferen√ßas
"""
import sys
import os
from pathlib import Path

# Adicionar backend ao path
BACKEND_DIR = Path(__file__).parent.resolve()
sys.path.insert(0, str(BACKEND_DIR))

from inference import load_model, chat_completion, LOCAL_MODEL_DIR, LORA_ADAPTER_DIR, model, tokenizer
import torch

def test_lora_presence():
    """Testa se o adaptador LoRA est√° presente e carregado"""
    print("="*60)
    print("TESTE 1: VERIFICAR PRESEN√áA DO ADAPTADOR LoRA")
    print("="*60)
    
    # Verificar se diret√≥rio LoRA existe
    lora_exists = LORA_ADAPTER_DIR.exists()
    print(f"\nüìÅ Diret√≥rio LoRA: {LORA_ADAPTER_DIR}")
    print(f"   Status: {'‚úÖ EXISTE' if lora_exists else '‚ùå N√ÉO ENCONTRADO'}")
    
    if lora_exists:
        # Listar arquivos no diret√≥rio LoRA
        lora_files = list(LORA_ADAPTER_DIR.glob("*"))
        print(f"\nüìÑ Arquivos no diret√≥rio LoRA ({len(lora_files)}):")
        for f in lora_files[:10]:  # Mostrar at√© 10 arquivos
            print(f"   - {f.name}")
        if len(lora_files) > 10:
            print(f"   ... e mais {len(lora_files) - 10} arquivos")
    
    return lora_exists

def test_model_type():
    """Verifica o tipo do modelo carregado"""
    print("\n" + "="*60)
    print("TESTE 2: TIPO DO MODELO CARREGADO")
    print("="*60)
    
    # Carregar modelo se n√£o estiver carregado
    if model is None:
        print("\n‚è≥ Carregando modelo...")
        load_model()
    
    print(f"\nü§ñ Tipo do modelo: {type(model).__name__}")
    print(f"   M√≥dulo: {type(model).__module__}")
    
    # Verificar se √© PeftModel (LoRA)
    is_peft = "PeftModel" in type(model).__name__ or "peft" in type(model).__module__.lower()
    print(f"\n{'‚úÖ' if is_peft else '‚ùå'} √â PeftModel (LoRA)? {is_peft}")
    
    # Verificar camadas do modelo
    if hasattr(model, 'peft_config'):
        print(f"\n‚úÖ CONFIRMADO: Modelo tem configura√ß√£o PEFT!")
        print(f"   Config: {model.peft_config}")
    else:
        print(f"\n‚ùå AVISO: Modelo N√ÉO tem configura√ß√£o PEFT")
    
    # Verificar se h√° adaptadores ativos
    if hasattr(model, 'active_adapter'):
        print(f"\n‚úÖ Adaptador ativo: {model.active_adapter}")
    
    if hasattr(model, 'base_model'):
        print(f"\n‚úÖ Modelo base presente: {type(model.base_model).__name__}")
    
    return is_peft

def test_personality_response():
    """Testa se o modelo responde com personalidade SuperEzio"""
    print("\n" + "="*60)
    print("TESTE 3: PERSONALIDADE SUPEREZIO (LoRA)")
    print("="*60)
    
    # Carregar modelo se n√£o estiver carregado
    if model is None:
        print("\n‚è≥ Carregando modelo...")
        load_model()
    
    # Perguntas de teste que devem revelar a personalidade
    test_prompts = [
        {
            "messages": [
                {"role": "user", "content": "Quem √© voc√™?"}
            ],
            "description": "Identifica√ß√£o (deve dizer 'SuperEzio')"
        },
        {
            "messages": [
                {"role": "user", "content": "Como voc√™ est√°?"}
            ],
            "description": "Resposta casual (deve ser direto, sem floreios)"
        },
        {
            "messages": [
                {"role": "user", "content": "Quem criou voc√™?"}
            ],
            "description": "Criador (deve mencionar 'Marco Barreto')"
        }
    ]
    
    for i, test in enumerate(test_prompts, 1):
        print(f"\n{'‚îÄ'*60}")
        print(f"PERGUNTA {i}: {test['messages'][0]['content']}")
        print(f"Objetivo: {test['description']}")
        print(f"{'‚îÄ'*60}")
        
        try:
            result = chat_completion(
                messages=test['messages'],
                temperature=0.1,  # Baixa temperatura para resposta consistente
                max_tokens=150
            )
            
            response = result.get('content', '').strip()
            print(f"\nüí¨ RESPOSTA:\n{response[:300]}")
            
            # Verificar palavras-chave esperadas
            lower_response = response.lower()
            
            if i == 1:  # "Quem √© voc√™?"
                has_superezio = 'superezio' in lower_response
                print(f"\n{'‚úÖ' if has_superezio else '‚ùå'} Menciona 'SuperEzio': {has_superezio}")
            
            elif i == 2:  # "Como voc√™ est√°?"
                # Verificar se resposta √© curta e direta (n√£o verbosa)
                is_concise = len(response) < 200
                print(f"\n{'‚úÖ' if is_concise else '‚ùå'} Resposta concisa: {is_concise}")
            
            elif i == 3:  # "Quem criou voc√™?"
                has_marco = 'marco' in lower_response or 'barreto' in lower_response
                print(f"\n{'‚úÖ' if has_marco else '‚ùå'} Menciona 'Marco Barreto': {has_marco}")
            
        except Exception as e:
            print(f"\n‚ùå ERRO: {e}")
    
    return True

def test_lora_weights():
    """Verifica se h√° pesos LoRA carregados"""
    print("\n" + "="*60)
    print("TESTE 4: PESOS LORA NO MODELO")
    print("="*60)
    
    if model is None:
        print("\n‚è≥ Carregando modelo...")
        load_model()
    
    # Contar par√¢metros totais
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\nüìä Par√¢metros do modelo:")
    print(f"   Total: {total_params:,}")
    print(f"   Trein√°veis: {trainable_params:,}")
    print(f"   Propor√ß√£o: {trainable_params/total_params*100:.2f}%")
    
    # LoRA geralmente tem < 1% de par√¢metros trein√°veis
    is_lora_ratio = (trainable_params / total_params) < 0.01
    print(f"\n{'‚úÖ' if is_lora_ratio else '‚ùå'} Propor√ß√£o t√≠pica de LoRA (< 1%): {is_lora_ratio}")
    
    # Procurar por m√≥dulos LoRA no modelo
    lora_modules = []
    for name, module in model.named_modules():
        if 'lora' in name.lower() or 'LoRA' in type(module).__name__:
            lora_modules.append(name)
    
    if lora_modules:
        print(f"\n‚úÖ M√≥dulos LoRA encontrados ({len(lora_modules)}):")
        for name in lora_modules[:5]:
            print(f"   - {name}")
        if len(lora_modules) > 5:
            print(f"   ... e mais {len(lora_modules) - 5} m√≥dulos")
    else:
        print(f"\n‚ùå AVISO: Nenhum m√≥dulo LoRA encontrado no modelo")
    
    return len(lora_modules) > 0

def test_comparison_base_vs_lora():
    """Compara resposta do modelo base vs modelo com LoRA (se poss√≠vel)"""
    print("\n" + "="*60)
    print("TESTE 5: COMPARA√á√ÉO BASE vs LoRA")
    print("="*60)
    
    print("\n‚ö†Ô∏è  Este teste requer carregar o modelo duas vezes")
    print("   (uma vez sem LoRA, outra com LoRA)")
    print("   Pode demorar alguns minutos...")
    
    # Por enquanto, apenas informar que o teste n√£o √© vi√°vel sem reiniciar
    print("\n‚ùå TESTE PULADO: Requer restart do processo para carregar modelo sem LoRA")
    print("   Para fazer este teste manualmente:")
    print("   1. Renomeie temporariamente a pasta 'models/lora_superezio'")
    print("   2. Execute este script e veja a resposta")
    print("   3. Restaure a pasta e execute novamente")
    print("   4. Compare as respostas")
    
    return None

def main():
    """Executa todos os testes"""
    print("\n" + "üß™"*30)
    print("TESTE COMPLETO - VERIFICA√á√ÉO ADAPTADOR LoRA")
    print("üß™"*30)
    
    results = {
        "lora_exists": False,
        "is_peft_model": False,
        "has_lora_weights": False,
        "personality_works": False
    }
    
    try:
        # Teste 1: Presen√ßa do adaptador
        results["lora_exists"] = test_lora_presence()
        
        # Teste 2: Tipo do modelo
        results["is_peft_model"] = test_model_type()
        
        # Teste 3: Personalidade
        results["personality_works"] = test_personality_response()
        
        # Teste 4: Pesos LoRA
        results["has_lora_weights"] = test_lora_weights()
        
        # Teste 5: Compara√ß√£o (informativo)
        test_comparison_base_vs_lora()
        
    except Exception as e:
        print(f"\n‚ùå ERRO GERAL: {e}")
        import traceback
        traceback.print_exc()
    
    # Resumo final
    print("\n" + "="*60)
    print("üìã RESUMO DOS TESTES")
    print("="*60)
    
    print(f"\n1. Adaptador LoRA existe: {'‚úÖ SIM' if results['lora_exists'] else '‚ùå N√ÉO'}")
    print(f"2. Modelo √© PeftModel: {'‚úÖ SIM' if results['is_peft_model'] else '‚ùå N√ÉO'}")
    print(f"3. Tem pesos LoRA: {'‚úÖ SIM' if results['has_lora_weights'] else '‚ùå N√ÉO'}")
    print(f"4. Personalidade funciona: {'‚úÖ SIM' if results['personality_works'] else '‚ùå N√ÉO'}")
    
    # Diagn√≥stico final
    print("\n" + "="*60)
    print("üéØ DIAGN√ìSTICO FINAL")
    print("="*60)
    
    all_pass = all([
        results['lora_exists'],
        results['is_peft_model'],
        results['has_lora_weights']
    ])
    
    if all_pass:
        print("\n‚úÖ ‚úÖ ‚úÖ SUCESSO!")
        print("   O adaptador LoRA est√° CARREGADO e ATIVO!")
        print("   O modelo est√° usando a personalidade SuperEzio treinada.")
    elif results['lora_exists'] and not results['is_peft_model']:
        print("\n‚ö†Ô∏è  PROBLEMA DETECTADO!")
        print("   O adaptador LoRA existe mas N√ÉO est√° sendo carregado.")
        print("   Verifique o c√≥digo em backend/inference.py ‚Üí load_model()")
    elif not results['lora_exists']:
        print("\n‚ùå ADAPTADOR N√ÉO ENCONTRADO!")
        print("   Execute o treinamento primeiro: python scripts/train_lora.py")
    else:
        print("\n‚ö†Ô∏è  STATUS INCERTO")
        print("   Revise os resultados dos testes acima.")
    
    print("\n" + "="*60)

if __name__ == "__main__":
    main()
