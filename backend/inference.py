"""
Hugging Face Inference Backend para SuperEzio
100% LOCAL - Modelo j√° baixado no disco, sem depend√™ncia do HF
Usa GPU NVIDIA (CUDA) para infer√™ncia local

CAMINHOS RELATIVOS - Funciona em qualquer m√°quina
"""
import os
import sys
import json
from pathlib import Path
from typing import List, Dict, Optional, Any, Generator, Union
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    pipeline,
    BitsAndBytesConfig,
    TextIteratorStreamer,
    PreTrainedModel,
    PreTrainedTokenizer,
)
from threading import Thread
import queue
import torch
from peft.peft_model import PeftModel

# Fix encoding para Windows (removido - causa problemas com venv)
# O Python 3.12+ j√° lida bem com UTF-8 no Windows
# Se precisar, configure PYTHONIOENCODING=utf-8 no ambiente

# ‚úÖ FIX: Usar HF_HOME ao inv√©s de TRANSFORMERS_CACHE (deprecated)
if not os.getenv("HF_HOME"):
    os.environ["HF_HOME"] = os.path.expanduser("~/.cache/huggingface")

# Configura√ß√£o - MODELO 100% LOCAL
# Caminho relativo: backend/ -> raiz do projeto -> models/
BACKEND_DIR = Path(__file__).parent.resolve()
PROJECT_ROOT = BACKEND_DIR.parent.resolve()
LOCAL_MODEL_DIR = PROJECT_ROOT / "models" / "qwen2.5-7b-instruct"

# üî• MULTI-LORA: Suporte para m√∫ltiplos adaptadores
LORA_PERSONALITY_DIR = PROJECT_ROOT / "models" / "lora_personality"  # üé≠ Personalidade
LORA_ACCOUNTING_DIR = PROJECT_ROOT / "models" / "lora_accounting"    # üá®üá¶ Contabilidade
LORA_LEGACY_DIR = PROJECT_ROOT / "models" / "lora_superezio"          # Legacy (antigo)

# Permitir override via env
env_path = os.getenv("LOCAL_MODEL_PATH")
if env_path:
    LOCAL_MODEL_DIR = Path(env_path).resolve()

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Verificar se modelo existe localmente
if not LOCAL_MODEL_DIR.exists():
    raise FileNotFoundError(
        f"‚ùå Modelo n√£o encontrado em {LOCAL_MODEL_DIR}\n"
        f"üì• Execute primeiro: python scripts/download_model.py\n"
        f"   Isso vai baixar o modelo UMA VEZ do Hugging Face.\n"
        f"   Depois disso, funciona 100% offline!"
    )

# Carregar modelo (uma vez, reutilizar)
tokenizer: Optional[PreTrainedTokenizer] = None
model: Optional[Union[PreTrainedModel, PeftModel]] = None
generator: Optional[Any] = None  # pipeline type √© complexo, usar Any

def load_model():
    """
    Carrega o modelo LOCAL na GPU.
    Se um adaptador LoRA treinado for encontrado, ele ser√° aplicado sobre o modelo base.
    """
    global tokenizer, model, generator
    
    if model is not None:
        return  # J√° carregado

    try:
        import torch  # Garantir torch dispon√≠vel localmente
        use_lora = LORA_PERSONALITY_DIR.exists() or LORA_ACCOUNTING_DIR.exists() or LORA_LEGACY_DIR.exists()

        # Carregar tokenizer (simples, sem LoRA)
        print(f"üìÇ Carregando tokenizer de {LOCAL_MODEL_DIR}...")
        tokenizer = AutoTokenizer.from_pretrained(
            str(LOCAL_MODEL_DIR),
            trust_remote_code=True,
            local_files_only=True,
        )
        tokenizer.pad_token = tokenizer.eos_token  # type: ignore

        if use_lora:
            # Detectar quais LoRAs existem
            has_personality = LORA_PERSONALITY_DIR.exists()
            has_accounting = LORA_ACCOUNTING_DIR.exists()
            has_legacy = LORA_LEGACY_DIR.exists()
            
            print("="*50)
            print("üöÄ MULTI-LORA DETECTADO! üöÄ")
            if has_personality:
                print(f"   üé≠ Personalidade: {LORA_PERSONALITY_DIR}")
            if has_accounting:
                print(f"   üá®üá¶ Contabilidade: {LORA_ACCOUNTING_DIR}")
            if has_legacy:
                print(f"   üì¶ Legacy: {LORA_LEGACY_DIR}")
            print("="*50)

            # Configura√ß√£o de quantiza√ß√£o para carregar o modelo base em 4-bit
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
            )

            # Carregar o modelo base j√° quantizado
            print(f"üìÇ Carregando modelo base ({LOCAL_MODEL_DIR}) em 4-bit...")
            model = AutoModelForCausalLM.from_pretrained(
                str(LOCAL_MODEL_DIR),
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True,
                local_files_only=True,
            )
            
            # üî• CARREGAR M√öLTIPLOS LoRAs
            print("üöÄ Aplicando adaptadores LoRA...")
            
            # Prioridade: Personality + Accounting > Legacy
            if has_personality and has_accounting:
                # Carregar ambos LoRAs
                model = PeftModel.from_pretrained(model, str(LORA_PERSONALITY_DIR), adapter_name="personality")
                model.load_adapter(str(LORA_ACCOUNTING_DIR), adapter_name="accounting")
                # Ativar ambos usando enable_adapter_layers()
                model.enable_adapter_layers()
                print("‚úÖ Multi-LoRA aplicado: üé≠ Personalidade + üá®üá¶ Contabilidade!")
                print("üé≠ Personalidade SuperEzio ATIVADA!")
                print("üá®üá¶ Expert em Contabilidade Canadense ATIVADO!")
                print("‚ÑπÔ∏è  Ambos adaptadores ser√£o mesclados nas respostas")
            elif has_personality:
                model = PeftModel.from_pretrained(model, str(LORA_PERSONALITY_DIR), is_trainable=False)
                print("‚úÖ LoRA de Personalidade aplicado!")
                print("üé≠ Personalidade SuperEzio ATIVADA!")
            elif has_accounting:
                model = PeftModel.from_pretrained(model, str(LORA_ACCOUNTING_DIR), is_trainable=False)
                print("‚úÖ LoRA de Contabilidade aplicado!")
                print("üá®üá¶ Expert em Contabilidade Canadense ATIVADO!")
            elif has_legacy:
                model = PeftModel.from_pretrained(model, str(LORA_LEGACY_DIR), is_trainable=False)
                print("‚úÖ LoRA Legacy aplicado!")
                print("üé≠ Personalidade SuperEzio ATIVADA!")

        else:
            print("="*50)
            print("üöÄ Adaptador LoRA n√£o encontrado. Carregando modelo base padr√£o. üöÄ")
            print(f"(Para treinar um, execute: python scripts/train_lora.py)")
            print("="*50)
            
            # Configura√ß√£o quantiza√ß√£o 4-bit (reduz VRAM, aumenta velocidade)
            print("üîß Configurando quantiza√ß√£o 4-bit...")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
            )
            
            # Carregar modelo do disco LOCAL com quantiza√ß√£o 4-bit
            print(f"üìÇ Carregando modelo de {LOCAL_MODEL_DIR}...")
            print("‚ö° Modo: 4-bit quantized (NF4)")
            model = AutoModelForCausalLM.from_pretrained(
                str(LOCAL_MODEL_DIR),
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True,
                local_files_only=True,
            )
            
            print("‚úÖ Modelo carregado com quantiza√ß√£o 4-bit")

        # Otimiza√ß√µes CUDA
        if DEVICE == "cuda":
            print("üîß Aplicando otimiza√ß√µes CUDA...")
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            
            # BetterTransformer para 30-50% mais r√°pido
            try:
                from optimum.bettertransformer import BetterTransformer
                model = BetterTransformer.transform(model)  # type: ignore
                print("‚úÖ BetterTransformer ativado (30-50% mais r√°pido)")
            except Exception as e:
                print(f"‚ö†Ô∏è  BetterTransformer n√£o dispon√≠vel: {e}")
            
            print("‚úÖ Otimiza√ß√µes CUDA ativadas")
            
            # NOTA: torch.compile desabilitado pois quebra o pipeline do HuggingFace
            # O modelo compila mas vira OptimizedModule que n√£o √© suportado pelo pipeline
            # Se quiser usar torch.compile, precisa usar model.generate() diretamente
        
        # Criar pipeline de gera√ß√£o de texto (n√£o funciona com torch.compile)
        generator = pipeline(  # type: ignore[call-overload]
            "text-generation",
            model=model,  # type: ignore[arg-type]
            tokenizer=tokenizer,
        )
        
        vram_used = torch.cuda.memory_allocated(0) / 1024**3 if DEVICE == "cuda" else 0
        vram_total = torch.cuda.get_device_properties(0).total_memory / (1024**3) if DEVICE == "cuda" else 0
        print(f"‚úÖ Modelo carregado 100% LOCAL!")
        print(f"üíæ VRAM: {vram_used:.2f}GB / {vram_total:.2f}GB")
        print(f"üåê Status: OFFLINE (sem depend√™ncia do Hugging Face)")
        
    except Exception as e:
        print(f"‚ùå Erro ao carregar modelo: {e}")
        import traceback
        traceback.print_exc()
        if "local_files_only" in str(e) or "not found" in str(e).lower():
            print(f"\nüí° Dica: Execute primeiro:")
            print(f"   python scripts/download_model.py")
        raise


# SYSTEM_PROMPT - Minimalista (personalidade j√° est√° no LoRA)
SYSTEM_PROMPT = """Voc√™ √© SuperEzio. Responda em portugu√™s brasileiro de forma direta e objetiva.

**FERRAMENTAS DISPON√çVEIS:**
Voc√™ tem acesso a ferramentas para ajudar o usu√°rio:
- **Arquivos:** read_file, write_file, delete_file, get_file_info
- **Diret√≥rios:** list_directory, create_directory, search_files  
- **Dados:** create_table
- **Email:** read_emails, search_emails, get_unread_count

**QUANDO USAR FERRAMENTAS:**
- Usu√°rio pede para ler/criar/modificar arquivos ‚Üí USE read_file, write_file
- Usu√°rio quer listar pastas/arquivos ‚Üí USE list_directory, search_files
- Usu√°rio quer ver emails ‚Üí USE read_emails, search_emails
- Usu√°rio pede para organizar dados ‚Üí USE create_table

**IMPORTANTE:**
- Se o usu√°rio pedir algo que REQUER uma ferramenta, SEMPRE use ela
- N√ÉO invente conte√∫do de arquivos - leia primeiro com read_file
- N√ÉO diga "eu n√£o posso" se existe uma ferramenta para isso
- Seja PROATIVO: se precisa de info de um arquivo, leia ele
- Use g√≠rias: "cara", "mano", "beleza?", "t√° ligado?", "saca?"
- Seja EXPRESSIVO: use emoji quando apropriado üòéüöÄüí™

SA√öDE (SENS√çVEL - S√ì ORGANIZAR QUANDO SOLICITADO):
- TDAH: Preferiria Ritalina, usa Vyvanse 40mg atualmente
- Ansiedade: Sono ruim, sertralina
- Hipertens√£o: Ramipril e metoprolol
- Card√≠aco: 4 abla√ß√µes por arritmia, est√°vel
- Sono: Ronco, CPAP
- Peso: ~134 kg, meta perda peso/energia, brain fog
- REGRA: N√£o prescrever, n√£o empurrar alertas; s√≥ organizar quando pedir

OBJETIVOS EM ANDAMENTO:
- SuperEzio: personalidade pr√≥pria, baixa lat√™ncia
- TrafficAI: competir com Miovision-like, ROI √≥timo
- Xubudget: consolidar uso familiar
- Trazer Matheus do Brasil para Canad√° (apoio log√≠stico)

DATAS-CHAVE:
- 2025-10-09: Sa√≠da Hayes/Instech
- 2025: Falecimento Inesita e Jos√© Carlos
- Di√°rio 20:00: Chamada AP com Matheus

DIRETRIZES:
- Sempre entregar pipeline completo (pasta ‚Üí arquivo ‚Üí comando)
- Sugerir modelos pequenos em paralelo + orquestrador
- Deixar claro custo/benef√≠cio e execu√ß√£o em 1 comando
- Em temas familiares, manter calor e objetividade
- Em sa√∫de: somente organizar quando solicitado, sem protocolos n√£o solicitados"""

def format_messages(messages: List[Dict[str, str]]) -> str:
    """Formata mensagens para o formato do modelo"""
    # Garantir que h√° uma mensagem system com o SYSTEM_PROMPT
    has_system = any(msg.get("role") == "system" for msg in messages)
    if not has_system:
        messages = [{"role": "system", "content": SYSTEM_PROMPT}] + messages
    
    # Usar chat template do modelo
    if tokenizer and hasattr(tokenizer, 'apply_chat_template'):
        result = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        return str(result)  # Garantir que retorna string
    
    # Fallback: formato simples
    formatted = ""
    for msg in messages:
        role = msg.get("role", "user")
        content = msg.get("content", "")
        if role == "system":
            formatted += f"System: {content}\n\n"
        elif role == "user":
            formatted += f"User: {content}\n\n"
        elif role == "assistant":
            formatted += f"Assistant: {content}\n\n"
    
    formatted += "Assistant: "
    return formatted

def generate_stream(
    messages: List[Dict[str, str]],
    max_new_tokens: int = 512,
    temperature: float = 0.2,
    top_p: float = 0.9,
    top_k: int = 40,
    repetition_penalty: float = 1.1,
    tools: Optional[List[Dict]] = None
) -> Generator[str, None, None]:
    """
    Stream de gera√ß√£o token por token.
    Yields: chunks de texto conforme modelo gera.
    """
    global model, tokenizer
    
    if model is None or tokenizer is None:
        load_model()
    
    # Type guard: garantir que model e tokenizer foram carregados
    if model is None or tokenizer is None:
        yield "[ERRO: Modelo n√£o carregado]"
        return
    
    try:
        # Formatar prompt
        prompt = format_messages(messages)
        
        if tools:
            tools_json = json.dumps(tools, indent=2)
            prompt += f"\n\n[AVAILABLE_TOOLS]\n{tools_json}\n\n"
            prompt += "Voc√™ pode chamar essas ferramentas quando necess√°rio. Responda em JSON com 'content' e 'tool_calls'.\n"
        
        # Tokenizar
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048,  # Reduzido de 4096 para 2048 (mais r√°pido)
            padding=False,  # Sem padding desnecess√°rio
        )
        
        # Mover para device correto
        if DEVICE == "cuda":
            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
        
        # Setup streamer
        streamer = TextIteratorStreamer(
            tokenizer,  # type: ignore[arg-type]
            skip_prompt=True,
            skip_special_tokens=True,
            timeout=10.0
        )
        
        generation_kwargs = {
            "input_ids": inputs["input_ids"],
            "attention_mask": inputs["attention_mask"],
            "max_new_tokens": max_new_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
            "repetition_penalty": repetition_penalty,
            "do_sample": temperature > 0,
            "num_beams": 1,
            "streamer": streamer,
            "pad_token_id": tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,
            "eos_token_id": tokenizer.eos_token_id,
            "use_cache": True,  # KV cache para performance
            "num_return_sequences": 1,
        }
        
        # Thread para gera√ß√£o n√£o-bloqueante
        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()
        
        # Yield tokens conforme chegam
        for text_chunk in streamer:
            if text_chunk:
                yield text_chunk
        
        thread.join()
        
    except Exception as e:
        print(f"‚ùå Erro no streaming: {e}")
        import traceback
        traceback.print_exc()
        yield f"[ERRO: {str(e)}]"
    finally:
        # Limpar cache CUDA se necess√°rio
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

def chat_completion(
    messages: List[Dict[str, str]],
    tools: Optional[List[Dict[str, Any]]] = None,
    temperature: float = 0.2,
    max_tokens: int = 2048,
    stream: bool = False
) -> Union[Dict[str, Any], Generator[str, None, None]]:
    """
    Chat completion com suporte a streaming.
    
    Args:
        messages: Lista de mensagens (formato OpenAI)
        tools: Lista de tools dispon√≠veis (function calling)
        temperature: Temperatura para gera√ß√£o
        max_tokens: M√°ximo de tokens a gerar
        stream: True = retorna generator, False = string completa
    
    Returns:
        Se stream=True: Generator[str, None, None]
        Se stream=False: Dict com 'content' e opcionalmente 'tool_calls'
    """
    if stream:
        return generate_stream(
            messages=messages,
            max_new_tokens=min(max_tokens, 512),
            temperature=temperature,
            tools=tools
        )
    
    import time
    global generator, tokenizer
    
    if generator is None:
        load_model()
    
    # Type guard: garantir que generator e tokenizer foram carregados
    if generator is None or tokenizer is None:
        return {
            "error": "Modelo n√£o carregado",
            "content": "Erro: Modelo n√£o est√° carregado. Reinicie o servidor."
        }
    
    # Log da pergunta do usu√°rio
    if messages:
        last_message = messages[-1].get('content', '')
        print(f"üó£Ô∏è  PERGUNTA: \"{last_message[:200]}{'...' if len(last_message) > 200 else ''}\"")

    print(f"üîß Formatando prompt...")
    start_format = time.time()
    
    # Formatar prompt
    try:
        prompt = format_messages(messages)
        format_time = time.time() - start_format
        print(f"‚úÖ Prompt formatado em {format_time:.2f}s")
        print(f"üìè Tamanho do prompt: {len(prompt)} caracteres")
        if len(prompt) > 10000:
            print(f"‚ö†Ô∏è  AVISO: Prompt muito longo ({len(prompt)} chars), pode demorar mais")
    except Exception as format_error:
        print(f"‚ùå Erro ao formatar prompt: {format_error}")
        raise
    
    # Se h√° tools, adicionar ao prompt (function calling)
    if tools:
        tools_json = json.dumps(tools, indent=2)
        prompt += f"\n\n[AVAILABLE_TOOLS]\n{tools_json}\n\n"
        prompt += "Voc√™ pode chamar essas ferramentas quando necess√°rio. Responda em JSON com 'content' e 'tool_calls'.\n"
        print(f"üîß Tools adicionadas ao prompt")
    
    try:
        # NOTA: max_tokens j√° vem limitado a 512 pelo api.py
        # N√£o precisa mais de verifica√ß√£o redundante aqui
        print(f"üöÄ Gerando com max_new_tokens={max_tokens}, temperature={temperature}")
        print(f"‚è±Ô∏è  Iniciando gera√ß√£o √†s {time.strftime('%H:%M:%S')}...")
        print(f"üìä Prompt length: {len(prompt)} chars")

        gen_start = time.time()
        
        # Verificar se modelo est√° carregado
        if generator is None:
            print("‚ùå ERRO: Generator n√£o est√° inicializado!")
            return {
                "error": "Modelo n√£o est√° carregado",
                "content": "Erro: Modelo n√£o est√° carregado. Reinicie o servidor FastAPI."
            }

        # Gera√ß√£o com par√¢metros otimizados
        try:
            print(f"üîÑ Chamando generator.generate()...")
            gen_call_start = time.time()
            outputs = generator(
                prompt,
                max_new_tokens=max_tokens,  # J√° limitado a 512 pelo api.py
                temperature=temperature,
                top_p=0.9,   # Reduzido para acelerar
                top_k=40,    # Reduzido para acelerar
                do_sample=True,
                num_beams=1,  # Sem beam search - mais r√°pido
                repetition_penalty=1.1,  # Evita repeti√ß√£o
                return_full_text=False,
                pad_token_id=tokenizer.eos_token_id,  # Evitar warnings
            )
            gen_call_time = time.time() - gen_call_start
            print(f"‚úÖ Generator retornou em {gen_call_time:.2f}s")
        except Exception as gen_error:
            gen_time = time.time() - gen_start
            print(f"‚ùå Erro durante gera√ß√£o ap√≥s {gen_time:.2f}s: {gen_error}")
            import traceback
            traceback.print_exc()
            raise
        
        gen_time = time.time() - gen_start
        print(f"‚úÖ Gera√ß√£o conclu√≠da em {gen_time:.2f}s")
        
        # Verificar se a resposta est√° vazia ou muito curta (pode indicar problema)
        if not outputs or len(outputs) == 0:
            print(f"‚ö†Ô∏è  AVISO: Gera√ß√£o retornou vazio!")
            return {
                "error": "Gera√ß√£o retornou resposta vazia",
                "content": "Desculpe, n√£o consegui gerar uma resposta. Tente novamente."
            }
        
        response_text = outputs[0]["generated_text"]
        
        # Log da resposta final
        print(f"üí° RESPOSTA: \"{response_text[:200].strip()}{'...' if len(response_text) > 200 else ''}\"")
        
        # REM: logar "resposta bruta" (primeiros 500 chars)
        try:
            preview = response_text[:500]
            print(f"üìù Resposta bruta do modelo ({len(response_text)} chars):")
            print(f"{'‚îÄ'*60}")
            print(f"{preview}{'...' if len(response_text) > 500 else ''}")
            print(f"{'‚îÄ'*60}")
        except Exception as e:
            print(f"‚ö†Ô∏è Log preview falhou: {e}")
        
        # Tentar parsear JSON se houver tool calls
        result = {"content": response_text, "tool_calls": None}
        
        if tools and "tool_calls" in response_text.lower():
            try:
                # Tentar extrair JSON da resposta
                json_match = response_text.find("{")
                if json_match != -1:
                    json_str = response_text[json_match:]
                    parsed = json.loads(json_str)
                    if "tool_calls" in parsed:
                        result["tool_calls"] = parsed["tool_calls"]
                        result["content"] = parsed.get("content", response_text)
            except:
                pass  # Se n√£o conseguir parsear, retorna texto normal
        
        return result
        
    except Exception as e:
        return {
            "error": str(e),
            "content": f"Erro ao gerar resposta: {e}"
        }

# Teste b√°sico
if __name__ == "__main__":
    load_model()
    
    test_messages = [
        {"role": "system", "content": "Voc√™ √© SuperEzio, um assistente direto e objetivo."},
        {"role": "user", "content": "Oi, como voc√™ est√°?"}
    ]
    
    result = chat_completion(test_messages, stream=False)  # Especificar stream=False
    print("\nüìù Resposta:")
    if isinstance(result, dict):
        print(result.get("content", "Erro: sem conte√∫do"))
    else:
        print("Erro: resultado n√£o √© dict")

