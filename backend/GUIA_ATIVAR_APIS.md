# Guia R√°pido: Ativar APIs Externas (17x Mais R√°pido)

## Problema Atual

Pipeline usando **Qwen fallback** para todos os 3 stages:
- ‚è±Ô∏è Stage 1: ~50s
- ‚è±Ô∏è Stage 2: ~60s
- ‚è±Ô∏è Stage 3: ~50s
- **Total: ~160s (2.7 minutos)**

## Solu√ß√£o: APIs Externas

Usar **HuggingFace Inference API** para Stage 2 e 3:
- ‚ö° Stage 1: ~2s (Qwen local)
- ‚ö° Stage 2: ~4s (DeepSeek-Coder API)
- ‚ö° Stage 3: ~3s (LLaMA 3 API)
- **Total: ~9s (17x mais r√°pido!)**

---

## Passo 1: Obter HuggingFace Token

### 1.1. Criar Conta (se n√£o tem)
```
https://huggingface.co/join
```

### 1.2. Gerar Token de Acesso
1. Acesse: https://huggingface.co/settings/tokens
2. Clique em **"New token"**
3. Nome: `superezio-code-pipeline`
4. Tipo: **Read** (suficiente para Inference API)
5. Copie o token: `hf_...`

---

## Passo 2: Configurar Vari√°vel de Ambiente

### Windows PowerShell (Tempor√°rio - Sess√£o Atual)

```powershell
$env:HF_TOKEN = "hf_SEU_TOKEN_AQUI"
```

### Windows PowerShell (Permanente)

```powershell
# Adicionar ao perfil do PowerShell
notepad $PROFILE

# Cole essa linha no arquivo:
$env:HF_TOKEN = "hf_SEU_TOKEN_AQUI"

# Salve e recarregue:
. $PROFILE
```

### Alternativa: Arquivo .env

Crie arquivo `.env` na raiz do projeto:

```bash
# .env
HF_TOKEN=hf_SEU_TOKEN_AQUI
```

E adicione no `backend/code_pipeline.py`:

```python
from dotenv import load_dotenv
load_dotenv()
```

---

## Passo 3: Implementar Chamadas de API

### 3.1. Instalar Depend√™ncia

```powershell
pip install requests
```

### 3.2. Adicionar Fun√ß√£o no code_pipeline.py

```python
import requests
import os

HF_TOKEN = os.getenv("HF_TOKEN")

def call_hf_inference_api(model_url: str, prompt: str, max_tokens: int = 1024) -> str:
    """
    Chama HuggingFace Inference API.
    
    Args:
        model_url: URL do modelo (ex: DeepSeek-Coder)
        prompt: Prompt completo
        max_tokens: M√°ximo de tokens
        
    Returns:
        Resposta do modelo
    """
    if not HF_TOKEN:
        raise ValueError("HF_TOKEN n√£o configurado")
    
    headers = {
        "Authorization": f"Bearer {HF_TOKEN}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": max_tokens,
            "temperature": 0.2,
            "return_full_text": False
        }
    }
    
    response = requests.post(model_url, headers=headers, json=payload, timeout=60)
    response.raise_for_status()
    
    result = response.json()
    
    # HF Inference API retorna lista
    if isinstance(result, list) and len(result) > 0:
        return result[0].get("generated_text", "")
    
    return result.get("generated_text", "")
```

### 3.3. Atualizar Stage 2 (Coder)

```python
def call_model_deepseek_coder(...):
    # Tentar API primeiro
    if HF_TOKEN:
        try:
            print(f"üöÄ [CODER] Using DeepSeek-Coder API")
            
            prompt = f"{CODER_SYSTEM_PROMPT}\n\n{user_query}\n\nPLAN: {plan_json}"
            
            response_text = call_hf_inference_api(
                model_url=DEEPSEEK_CODER_API,
                prompt=prompt,
                max_tokens=2048
            )
            
            # Parse JSON...
            
        except Exception as e:
            print(f"‚ö†Ô∏è  [CODER] API failed: {e}, using Qwen fallback")
    
    # Fallback para Qwen local
    print(f"‚ö†Ô∏è  [CODER] Using Qwen fallback")
    # ... c√≥digo existente ...
```

### 3.4. Atualizar Stage 3 (Reviewer)

```python
def call_model_llama_reviewer(...):
    # Tentar API primeiro
    if HF_TOKEN:
        try:
            print(f"üöÄ [REVIEWER] Using LLaMA 3 API")
            
            prompt = f"{REVIEWER_SYSTEM_PROMPT}\n\nPLAN: {plan_json}\n\nCODE: {code_json}"
            
            response_text = call_hf_inference_api(
                model_url=LLAMA3_API,
                prompt=prompt,
                max_tokens=2048
            )
            
            # Parse JSON...
            
        except Exception as e:
            print(f"‚ö†Ô∏è  [REVIEWER] API failed: {e}, using Qwen fallback")
    
    # Fallback para Qwen local
    print(f"‚ö†Ô∏è  [REVIEWER] Using Qwen fallback")
    # ... c√≥digo existente ...
```

---

## Passo 4: Testar

```powershell
# 1. Configurar token
$env:HF_TOKEN = "hf_SEU_TOKEN_AQUI"

# 2. Testar pipeline
cd backend
python test_code_pipeline.py

# 3. Ver logs
# Deve mostrar:
# üöÄ [CODER] Using DeepSeek-Coder API
# üöÄ [REVIEWER] Using LLaMA 3 API
```

---

## Passo 5: Rodar Backend

```powershell
cd backend
python api.py

# Em outro terminal:
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Crie fun√ß√£o Python fatorial"}]
  }'
```

---

## Performance Esperada

### Com APIs Ativadas ‚úÖ

```
[PIPELINE][PLANNER] Stage 1: Planning
  Expert: code_python
  ‚úÖ Plan generated in 2000ms

[PIPELINE][CODER] Stage 2: Code Generation
  üöÄ Using DeepSeek-Coder API
  ‚úÖ Code generated in 4000ms

[PIPELINE][REVIEWER] Stage 3: Code Review
  üöÄ Using LLaMA 3 API
  ‚úÖ Review completed in 3000ms

################################
# PIPELINE COMPLETED
# Total time: 9000ms (9s)
################################
```

### Sem APIs (Fallback) ‚ö†Ô∏è

```
[PIPELINE][PLANNER] Stage 1: Planning
  ‚úÖ Plan generated in 50000ms

[PIPELINE][CODER] Stage 2: Code Generation
  ‚ö†Ô∏è  Using Qwen fallback
  ‚úÖ Code generated in 60000ms

[PIPELINE][REVIEWER] Stage 3: Code Review
  ‚ö†Ô∏è  Using Qwen fallback
  ‚úÖ Review completed in 50000ms

################################
# PIPELINE COMPLETED
# Total time: 160000ms (160s)
################################
```

---

## Alternativa: OpenRouter

Se preferir usar **OpenRouter** (suporta mais modelos):

### 1. Obter API Key
```
https://openrouter.ai/keys
```

### 2. Configurar
```powershell
$env:OPENROUTER_API_KEY = "sk-or-..."
```

### 3. Implementar em code_pipeline.py
```python
OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions"

def call_openrouter(model: str, messages: list) -> str:
    headers = {
        "Authorization": f"Bearer {OPENROUTER_TOKEN}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": model,
        "messages": messages
    }
    
    response = requests.post(OPENROUTER_API_URL, headers=headers, json=payload)
    result = response.json()
    
    return result["choices"][0]["message"]["content"]
```

---

## Custos Estimados

### HuggingFace Inference API (Gr√°tis!)
- **DeepSeek-Coder**: Gr√°tis (rate limited)
- **LLaMA 3-8B**: Gr√°tis (rate limited)
- **Limite:** ~1000 requests/dia

### OpenRouter (Pago)
- **DeepSeek-Coder-6.7B**: $0.14/1M tokens
- **LLaMA 3-8B**: $0.18/1M tokens
- **Por query:** ~$0.001-0.003 (1-3 centavos)

---

## Troubleshooting

### Erro: "Model is currently loading"
```
‚ö†Ô∏è  Modelo ainda carregando no HF. Tente em 30s.
```
**Solu√ß√£o:** Aguarde ou use fallback autom√°tico.

### Erro: "Rate limit exceeded"
```
‚ö†Ô∏è  Rate limit excedido. Usando fallback.
```
**Solu√ß√£o:** Aguarde 1 minuto ou configure OpenRouter.

### Erro: "Invalid token"
```
‚ùå HF_TOKEN inv√°lido
```
**Solu√ß√£o:** Verifique token em https://huggingface.co/settings/tokens

---

## Conclus√£o

### Sem APIs (Padr√£o)
- ‚úÖ Funciona sempre
- ‚ö†Ô∏è Lento (~160s)
- ‚úÖ Zero custo

### Com APIs Externas
- ‚úÖ 17x mais r√°pido (~9s)
- ‚úÖ Modelos especializados
- ‚ö†Ô∏è Requer token (gr√°tis)

**Recomenda√ß√£o:** Ative APIs externas para melhor experi√™ncia!

---

**Pr√≥ximo Passo:** Implementar chamadas de API em `code_pipeline.py`
