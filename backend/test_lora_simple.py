"""
Teste Simples - Verificar LoRA sem importar inference completo
"""
import os
import sys
from pathlib import Path
import torch

# Configura√ß√£o
BACKEND_DIR = Path(__file__).parent.resolve()
PROJECT_ROOT = BACKEND_DIR.parent.resolve()
LOCAL_MODEL_DIR = PROJECT_ROOT / "models" / "qwen2.5-7b-instruct"
LORA_ADAPTER_DIR = PROJECT_ROOT / "models" / "lora_superezio"

print("="*60)
print("üß™ TESTE SIMPLES - VERIFICA√á√ÉO LoRA")
print("="*60)

# Teste 1: Verificar diret√≥rios
print("\nüìÅ TESTE 1: VERIFICAR DIRET√ìRIOS")
print("-"*60)

model_exists = LOCAL_MODEL_DIR.exists()
lora_exists = LORA_ADAPTER_DIR.exists()

print(f"Modelo base: {LOCAL_MODEL_DIR}")
print(f"  Status: {'‚úÖ EXISTE' if model_exists else '‚ùå N√ÉO ENCONTRADO'}")

print(f"\nAdaptador LoRA: {LORA_ADAPTER_DIR}")
print(f"  Status: {'‚úÖ EXISTE' if lora_exists else '‚ùå N√ÉO ENCONTRADO'}")

if lora_exists:
    lora_files = list(LORA_ADAPTER_DIR.glob("*"))
    print(f"\n  üìÑ Arquivos no diret√≥rio LoRA ({len(lora_files)}):")
    for f in sorted(lora_files)[:10]:
        size = f.stat().st_size if f.is_file() else 0
        size_mb = size / (1024*1024)
        print(f"     - {f.name} ({size_mb:.2f} MB)")
    
    # Verificar arquivos importantes
    adapter_config = LORA_ADAPTER_DIR / "adapter_config.json"
    adapter_model = LORA_ADAPTER_DIR / "adapter_model.safetensors"
    
    print(f"\n  üîç Arquivos cr√≠ticos:")
    print(f"     adapter_config.json: {'‚úÖ' if adapter_config.exists() else '‚ùå'}")
    print(f"     adapter_model.safetensors: {'‚úÖ' if adapter_model.exists() else '‚ùå'}")

# Teste 2: Carregar e verificar modelo
if model_exists:
    print("\n\nü§ñ TESTE 2: CARREGAR MODELO")
    print("-"*60)
    
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
        from peft.peft_model import PeftModel
        
        print("‚úÖ Imports bem-sucedidos")
        
        # Configura√ß√£o de quantiza√ß√£o
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )
        
        print("\n‚è≥ Carregando modelo base (4-bit)...")
        base_model = AutoModelForCausalLM.from_pretrained(
            str(LOCAL_MODEL_DIR),
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            local_files_only=True,
        )
        print("‚úÖ Modelo base carregado")
        
        # Verificar tipo
        print(f"\n   Tipo: {type(base_model).__name__}")
        print(f"   M√≥dulo: {type(base_model).__module__}")
        
        # Tentar carregar LoRA
        if lora_exists:
            print("\n‚è≥ Carregando adaptador LoRA...")
            try:
                model = PeftModel.from_pretrained(
                    base_model, 
                    str(LORA_ADAPTER_DIR),
                    is_trainable=False
                )
                print("‚úÖ Adaptador LoRA carregado!")
                
                print(f"\n   Tipo ap√≥s LoRA: {type(model).__name__}")
                print(f"   M√≥dulo: {type(model).__module__}")
                
                # Verificar se √© PeftModel
                is_peft = "PeftModel" in type(model).__name__
                print(f"\n   {'‚úÖ' if is_peft else '‚ùå'} √â PeftModel: {is_peft}")
                
                # Verificar configura√ß√£o PEFT
                if hasattr(model, 'peft_config'):
                    print(f"   ‚úÖ Tem peft_config")
                    print(f"   Adaptadores: {list(model.peft_config.keys())}")
                
                if hasattr(model, 'active_adapter'):
                    print(f"   ‚úÖ Adaptador ativo: {model.active_adapter}")
                
                # Contar par√¢metros
                total_params = sum(p.numel() for p in model.parameters())
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                
                print(f"\n   üìä Par√¢metros:")
                print(f"      Total: {total_params:,}")
                print(f"      Trein√°veis: {trainable_params:,}")
                print(f"      Propor√ß√£o: {trainable_params/total_params*100:.4f}%")
                
                # Procurar m√≥dulos LoRA
                lora_modules = []
                for name, module in model.named_modules():
                    if 'lora' in name.lower():
                        lora_modules.append(name)
                
                if lora_modules:
                    print(f"\n   ‚úÖ M√≥dulos LoRA encontrados: {len(lora_modules)}")
                    print(f"      Exemplos:")
                    for name in lora_modules[:3]:
                        print(f"      - {name}")
                else:
                    print(f"\n   ‚ö†Ô∏è  Nenhum m√≥dulo 'lora' encontrado nos nomes")
                
                # TESTE DE INFER√äNCIA
                print("\n\nüí¨ TESTE 3: INFER√äNCIA COM LoRA")
                print("-"*60)
                
                tokenizer = AutoTokenizer.from_pretrained(
                    str(LOCAL_MODEL_DIR),
                    trust_remote_code=True,
                    local_files_only=True,
                )
                tokenizer.pad_token = tokenizer.eos_token
                
                # Prompt de teste
                test_prompt = "Quem √© voc√™?"
                
                print(f"Pergunta: {test_prompt}")
                print("\n‚è≥ Gerando resposta...")
                
                inputs = tokenizer(test_prompt, return_tensors="pt").to(model.device)
                
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.1,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                )
                
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                response_only = response[len(test_prompt):].strip()
                
                print(f"\nüí¨ Resposta:\n{response_only}")
                
                # Verificar se menciona SuperEzio
                has_superezio = 'superezio' in response_only.lower()
                print(f"\n{'‚úÖ' if has_superezio else '‚ö†Ô∏è'} Menciona 'SuperEzio': {has_superezio}")
                
            except Exception as e:
                print(f"‚ùå Erro ao carregar LoRA: {e}")
                import traceback
                traceback.print_exc()
        else:
            print("\n‚ö†Ô∏è  Adaptador LoRA n√£o encontrado - usando apenas modelo base")
        
    except Exception as e:
        print(f"\n‚ùå Erro: {e}")
        import traceback
        traceback.print_exc()

# Resumo
print("\n\n" + "="*60)
print("üìã RESUMO")
print("="*60)
print(f"\n1. Modelo base existe: {'‚úÖ' if model_exists else '‚ùå'}")
print(f"2. Adaptador LoRA existe: {'‚úÖ' if lora_exists else '‚ùå'}")

if model_exists and lora_exists:
    print("\n‚úÖ Ambos est√£o presentes - execute o servidor para testar!")
elif not lora_exists:
    print("\n‚ö†Ô∏è  Adaptador LoRA n√£o encontrado")
    print("   Execute: python scripts/train_lora.py")
else:
    print("\n‚ö†Ô∏è  Revise os erros acima")

print("\n" + "="*60)
