"""
Script para atualizar backend/inference.py para usar o Mega LoRA Unificado
Remove multi-LoRA logic e SYSTEM_PROMPT (agora tudo est√° no LoRA)
"""
import re
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent
INFERENCE_FILE = PROJECT_ROOT / "backend" / "inference.py"

print("="*80)
print("üîß ATUALIZANDO BACKEND PARA MEGA LoRA UNIFICADO")
print("="*80)
print(f"üìÅ Arquivo: {INFERENCE_FILE}")
print()

# Ler arquivo
with open(INFERENCE_FILE, "r", encoding="utf-8") as f:
    content = f.read()

# Backup
backup_file = INFERENCE_FILE.with_suffix(".py.backup")
with open(backup_file, "w", encoding="utf-8") as f:
    f.write(content)
print(f"üíæ Backup criado: {backup_file}")

# 1. Atualizar configura√ß√£o de LoRA
print("\n1Ô∏è‚É£  Atualizando configura√ß√£o de LoRA...")
old_lora_config = """# Multi-LoRA configuration
LORA_PERSONALITY_DIR = PROJECT_ROOT / "models" / "lora_personality_v2"  # üé≠
LORA_ACCOUNTING_DIR = PROJECT_ROOT / "models" / "lora_accounting"    # üá®üá¶
LORA_LEGACY_DIR = PROJECT_ROOT / "models" / "lora_superezio"          # Legacy"""

new_lora_config = """# MEGA LoRA Unificado - Personalidade + Contabilidade + Ferramentas
LORA_MEGA_UNIFIED_DIR = PROJECT_ROOT / "models" / "lora_mega_unified"  # üöÄ MEGA"""

content = content.replace(old_lora_config, new_lora_config)
print("   ‚úÖ Configura√ß√£o de LoRA atualizada")

# 2. Remover/comentar SYSTEM_PROMPT
print("\n2Ô∏è‚É£  Comentando SYSTEM_PROMPT (agora est√° no LoRA)...")
# Encontrar e comentar todo o bloco SYSTEM_PROMPT
system_prompt_pattern = r'(SYSTEM_PROMPT = """.*?""")'
content = re.sub(
    system_prompt_pattern,
    lambda m: '\n'.join(['# ' + line for line in m.group(1).split('\n')]),
    content,
    flags=re.DOTALL
)
print("   ‚úÖ SYSTEM_PROMPT comentado")

# 3. Simplificar l√≥gica de carregamento de LoRA
print("\n3Ô∏è‚É£  Simplificando l√≥gica de carregamento...")
# Substituir toda a l√≥gica de multi-LoRA por carregamento simples
old_loading_logic_pattern = r'# Configurar e carregar LoRA.*?print\("‚úÖ Modelo LoRA carregado"\)'

new_loading_logic = '''# Carregar MEGA LoRA Unificado
print("üîß Carregando MEGA LoRA Unificado...")
if LORA_MEGA_UNIFIED_DIR.exists():
    print(f"   üìÅ LoRA: {LORA_MEGA_UNIFIED_DIR.name}")
    model = PeftModel.from_pretrained(
        model,
        str(LORA_MEGA_UNIFIED_DIR),
        is_trainable=False,
    )
    print("‚úÖ MEGA LoRA carregado!")
    print("   üé≠ Personalidade SuperEzio")
    print("   üá®üá¶ Conhecimento Contabilidade CRA")
    print("   üîß 11 Ferramentas integradas")
else:
    print("‚ö†Ô∏è  MEGA LoRA n√£o encontrado, usando modelo base")
print("‚úÖ Modelo LoRA carregado")'''

content = re.sub(
    old_loading_logic_pattern,
    new_loading_logic,
    content,
    flags=re.DOTALL
)
print("   ‚úÖ L√≥gica de carregamento simplificada")

# 4. Atualizar fun√ß√£o generate() para n√£o usar SYSTEM_PROMPT
print("\n4Ô∏è‚É£  Atualizando fun√ß√£o generate()...")
# Remover uso de SYSTEM_PROMPT na fun√ß√£o generate
old_generate_pattern = r'messages = \[\{"role": "system", "content": SYSTEM_PROMPT\}\]'
new_generate_pattern = 'messages = []  # Sem system prompt - tudo est√° no LoRA!'
content = content.replace(old_generate_pattern, new_generate_pattern)
print("   ‚úÖ Fun√ß√£o generate() atualizada")

# Salvar arquivo atualizado
with open(INFERENCE_FILE, "w", encoding="utf-8") as f:
    f.write(content)

print()
print("="*80)
print("‚úÖ BACKEND ATUALIZADO COM SUCESSO!")
print("="*80)
print()
print("üìù MUDAN√áAS APLICADAS:")
print("   1. ‚úÖ Configura√ß√£o de LoRA: lora_mega_unified")
print("   2. ‚úÖ SYSTEM_PROMPT comentado")
print("   3. ‚úÖ Multi-LoRA removido ‚Üí Single MEGA LoRA")
print("   4. ‚úÖ Fun√ß√£o generate() sem system prompt")
print()
print("üöÄ PR√ìXIMOS PASSOS:")
print("   1. Verificar se o training completou")
print("   2. Reiniciar backend:")
print("      cd backend")
print("      python api.py")
print()
print("="*80)
