"""
LoRA Training Script - SuperEzio Personality Adapter
Treina adaptador LoRA customizado para personalidade SuperEzio

OTIMIZA√á√ïES 2025:
- QLoRA (4-bit) para RTX 3060 12GB
- Rank adaptativo baseado em Qwen2.5
- Target modules otimizados para arquitetura Qwen
- Hyperpar√¢metros testados e validados
"""
import os
import sys
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from peft import (
    get_peft_model,
    prepare_model_for_kbit_training,
    LoraConfig,
)
from trl import SFTTrainer  # type: ignore
from datasets import load_dataset
from pathlib import Path
import time

print("="*80)
print("üöÄ SUPEREZIO LoRA TRAINING v2.0 - OTIMIZADO 2025")
print("="*80)
print(f"‚è∞ In√≠cio: {time.strftime('%Y-%m-%d %H:%M:%S')}")
print()

# Configura√ß√µes
SCRIPT_DIR = Path(__file__).parent.resolve()
PROJECT_ROOT = SCRIPT_DIR.parent.resolve()
MODEL_PATH = PROJECT_ROOT / "models" / "qwen2.5-7b-instruct"
DEFAULT_DATASET = PROJECT_ROOT / "data" / "superezio_identity_balanced.jsonl"  # ÔøΩ 21 exemplos PERSONALIDADE
PERSONA_DATASET = PROJECT_ROOT / "data" / "persona_superezio_full.jsonl"  # üé≠ 110 exemplos personalidade
CRA_DATASET = PROJECT_ROOT / "data" / "cra_training.jsonl"  # üá®üá¶ 172 exemplos contabilidade
LEGACY_DATASET = PROJECT_ROOT / "data" / "persona_superezio.jsonl"

env_dataset = os.getenv("PERSONA_DATA_PATH")
if env_dataset:
    DATA_PATH = Path(env_dataset).expanduser().resolve()
elif DEFAULT_DATASET.exists():
    DATA_PATH = DEFAULT_DATASET
else:
    DATA_PATH = LEGACY_DATASET

OUTPUT_DIR = PROJECT_ROOT / "models" / "lora_personality_v2"  # ÔøΩ RETREINAMENTO ANTI-OVERFIT
LOG_DIR = PROJECT_ROOT / "logs" / "training"

print(f"üìÅ Modelo base: {MODEL_PATH}")
dataset_source = "env:PERSONA_DATA_PATH" if env_dataset else ("combined" if DATA_PATH == DEFAULT_DATASET else "legacy")
print(f"üìÑ Dataset: {DATA_PATH} ({dataset_source})")
print(f"üíæ Output: {OUTPUT_DIR}")
print(f"üìä Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
if torch.cuda.is_available():
    print(f"üéÆ GPU: {torch.cuda.get_device_name(0)}")
    print(f"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
print()

# Verifica√ß√µes
if not MODEL_PATH.exists():
    print(f"‚ùå Modelo n√£o encontrado: {MODEL_PATH}")
    sys.exit(1)

if not DATA_PATH.exists():
    print(f"‚ùå Dataset n√£o encontrado: {DATA_PATH}")
    sys.exit(1)

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)

# Quantiza√ß√£o QLoRA (4-bit) - Otimizada para treino eficiente
print("üîß Configurando QLoRA (4-bit quantization)...")
print("   ‚Ä¢ Tipo: NF4 (Normal Float 4-bit)")
print("   ‚Ä¢ Double quantization: Ativado")
print("   ‚Ä¢ Compute dtype: BFloat16")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,  # BFloat16 melhor que Float16
    bnb_4bit_use_double_quant=True,
)
print("‚úÖ Configura√ß√£o QLoRA pronta")
print()

# 3. Carregar modelo com quantiza√ß√£o
print("\nüì¶ Carregando modelo base...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    quantization_config=bnb_config,  # Usar bnb_config definido acima
    device_map={"": 0},  # For√ßar tudo na GPU 0
    trust_remote_code=True,
    dtype=torch.bfloat16,  # ‚úÖ CORRIGIDO: dtype ao inv√©s de torch_dtype
    low_cpu_mem_usage=True,
)

tokenizer = AutoTokenizer.from_pretrained(
    str(MODEL_PATH),
    trust_remote_code=True,
    local_files_only=True
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

model.config.pad_token_id = tokenizer.pad_token_id
print("‚úÖ Modelo carregado")

# Preparar para treino LoRA
print("üîß Preparando modelo para treino LoRA...")
model.gradient_checkpointing_enable()  # Economizar VRAM
model = prepare_model_for_kbit_training(model)
print("‚úÖ Modelo preparado para QLoRA")
print()

# Configura√ß√£o LoRA OTIMIZADA para Qwen2.5-7B
# ANTI-OVERFIT: Mais dropout, menos rank para datasets pequenos
print("‚ö° Configurando LoRA adapter...")
print("   üìä Par√¢metros otimizados para Qwen2.5:")
print("   ‚Ä¢ Rank (r): 16 (reduzido para evitar overfit)")
print("   ‚Ä¢ Alpha: 32 (2√órank)")
print("   ‚Ä¢ Target modules: Query, Key, Value, Output, MLP")
print("   ‚Ä¢ Dropout: 0.15 (ALTO para prevenir memoriza√ß√£o)")

lora_config = LoraConfig(
    r=16,                          # Rank 16 - menor para datasets pequenos
    lora_alpha=32,                 # Alpha = 2*r (recomendado)
    target_modules=[               # Todos os m√≥dulos cr√≠ticos do Qwen2.5
        "q_proj",                  # Query projection
        "k_proj",                  # Key projection
        "v_proj",                  # Value projection  
        "o_proj",                  # Output projection
        "gate_proj",               # MLP gate
        "up_proj",                 # MLP up
        "down_proj",               # MLP down
    ],
    lora_dropout=0.15,             # Dropout ALTO para prevenir overfit
    bias="none",                   # Sem bias adicional
    task_type="CAUSAL_LM",
    inference_mode=False,
    modules_to_save=None,          # N√£o salvar outros m√≥dulos
)

model = get_peft_model(model, lora_config)

# Mostrar estat√≠sticas de par√¢metros
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print()
print("üìä Estat√≠sticas do modelo:")
print(f"   Total de par√¢metros: {total_params:,}")
print(f"   Par√¢metros trein√°veis: {trainable_params:,}")
print(f"   Porcentagem trein√°vel: {100 * trainable_params / total_params:.2f}%")
print()
model.print_trainable_parameters()

# Carregar dataset
print("üìö Carregando dataset...")
dataset = load_dataset("json", data_files=str(DATA_PATH), split="train")

# Type guard: garantir que √© Dataset (n√£o DatasetDict)
from datasets import Dataset
if not isinstance(dataset, Dataset):
    print(f"‚ùå Erro: Dataset inv√°lido (tipo: {type(dataset)})")
    sys.exit(1)

print(f"‚úÖ {len(dataset)} exemplos carregados")
print()

# Fun√ß√£o de formata√ß√£o para treino
def format_instruction(sample):
    """Formata conversas no formato do modelo"""
    messages = sample["messages"]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False
    )
    return {"text": text}

# Aplicar formata√ß√£o
dataset = dataset.map(format_instruction)

# Argumentos de treino OTIMIZADOS
print("üéØ Configurando hyperpar√¢metros de treino...")
print("   üîÑ √âpocas: 5 (REDUZIDO para evitar overfit)")
print("   üì¶ Batch size: 1 (EST√ÅVEL - evita travamento)")
print("   üìà Learning rate: 2e-4 (padr√£o QLoRA)")
print("   üìâ Scheduler: Cosine com warmup")
print("   üíæ Checkpoint: A cada √©poca")

training_args = TrainingArguments(
    output_dir=str(OUTPUT_DIR),
    num_train_epochs=5,                    # 5 √©pocas (anti-overfit)
    per_device_train_batch_size=1,         # Batch=1 EST√ÅVEL
    gradient_accumulation_steps=8,         # Simula batch=8 (1x8)
    learning_rate=2e-4,                    # Learning rate padr√£o QLoRA
    lr_scheduler_type="cosine",            # Cosine decay (suave)
    warmup_ratio=0.03,                     # 3% warmup
    logging_steps=1,                       # Log cada step
    logging_dir=str(LOG_DIR),
    save_strategy="epoch",                 # Salvar a cada √©poca
    save_total_limit=2,                    # Manter √∫ltimos 2 checkpoints
    bf16=torch.cuda.is_bf16_supported(),   # BFloat16 se dispon√≠vel
    fp16=not torch.cuda.is_bf16_supported(), # Float16 como fallback
    optim="paged_adamw_8bit",              # Otimizador 8-bit (economiza VRAM)
    gradient_checkpointing=True,           # Economiza VRAM
    max_grad_norm=0.3,                     # Gradient clipping (previne explos√£o)
    weight_decay=0.001,                    # Regulariza√ß√£o
    report_to="none",                      # Sem integra√ß√£o wandb/tensorboard
    dataloader_num_workers=0,              # Windows: 0 workers
    remove_unused_columns=True,
    run_name=f"lora_superezio_{time.strftime('%Y%m%d_%H%M%S')}",
)
print("‚úÖ Configura√ß√£o de treino pronta")
print()

# Trainer com SFT (Supervised Fine-Tuning)
print("üèãÔ∏è Criando trainer...")
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    processing_class=tokenizer,
)
print("‚úÖ Trainer configurado")
print()

# TREINAR!
print("="*80)
print("üöÄ INICIANDO TREINAMENTO...")
print("="*80)
print(f"‚è∞ Hor√°rio: {time.strftime('%H:%M:%S')}")
print(f"üìä Total de exemplos: {len(dataset)}")
print(f"üì¶ Steps por √©poca: {len(dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}")
print(f"‚è±Ô∏è  Tempo estimado: ~15-30 minutos (RTX 3060)")
print("="*80)
print()

start_time = time.time()

try:
    trainer.train()
    training_time = time.time() - start_time
    
    print()
    print("="*80)
    print("‚úÖ TREINO COMPLETO!")
    print("="*80)
    print(f"‚è±Ô∏è  Tempo total: {training_time/60:.1f} minutos")
    print()
    
except KeyboardInterrupt:
    print("\n‚ö†Ô∏è  Treino interrompido pelo usu√°rio")
    print("üíæ Salvando progresso atual...")
except Exception as e:
    print(f"\n‚ùå Erro durante treino: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Salvar adapter final
print("üíæ Salvando LoRA adapter...")
model.save_pretrained(str(OUTPUT_DIR))
tokenizer.save_pretrained(str(OUTPUT_DIR))

# Salvar configura√ß√£o adicional
import json
config_info = {
    "model_base": "Qwen/Qwen2.5-7B-Instruct",
    "lora_rank": lora_config.r,
    "lora_alpha": lora_config.lora_alpha,
    "target_modules": list(lora_config.target_modules) if isinstance(lora_config.target_modules, set) else lora_config.target_modules,  # Converter set para list
    "training_time": f"{(time.time() - start_time)/60:.1f} min",
    "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
    "num_epochs": training_args.num_train_epochs,
    "dataset_size": len(dataset),
}

with open(OUTPUT_DIR / "training_info.json", "w", encoding="utf-8") as f:
    json.dump(config_info, f, indent=2, ensure_ascii=False)

print("‚úÖ LoRA adapter salvo com sucesso!")
print()

# Estat√≠sticas finais
adapter_size = sum(f.stat().st_size for f in OUTPUT_DIR.glob("**/*") if f.is_file())
print("="*80)
print("üìä ESTAT√çSTICAS FINAIS")
print("="*80)
print(f"üìÅ Localiza√ß√£o: {OUTPUT_DIR}")
print(f"üíæ Tamanho do adapter: {adapter_size / (1024**2):.1f} MB")
print(f"‚è±Ô∏è  Tempo total: {(time.time() - start_time)/60:.1f} minutos")
print(f"ÔøΩ Par√¢metros trein√°veis: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
print()
print("="*80)
print("üéâ SUPEREZIO PERSONALIZADO PRONTO!")
print("="*80)
print()
print("üìù PR√ìXIMOS PASSOS:")
print()
print("   1Ô∏è‚É£  Reinicie o backend Python:")
print("      cd backend")
print("      venv\\Scripts\\python.exe api.py")
print()
print("   2Ô∏è‚É£  O adapter ser√° carregado automaticamente")
print("      Voc√™ ver√°: 'üöÄ ADAPTADOR LoRA ENCONTRADO!'")
print()
print("   3Ô∏è‚É£  Teste a personalidade:")
print("      Pergunte: 'Quem √© voc√™?'")
print("      Resposta esperada: Men√ß√£o a 'SuperEzio'")
print()
print("="*80)
print(f"‚è∞ Fim: {time.strftime('%Y-%m-%d %H:%M:%S')}")
print("="*80)
