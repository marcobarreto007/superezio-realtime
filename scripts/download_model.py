"""
Script para baixar modelo do Hugging Face UMA VEZ
Depois disso, modelo fica 100% local e funciona offline
"""
import os
import sys
from pathlib import Path
from huggingface_hub import snapshot_download
from transformers import AutoTokenizer, AutoModelForCausalLM

# Fix encoding para Windows
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# Configura√ß√£o
HF_TOKEN = os.getenv("HUGGINGFACE_API_TOKEN", None)  # ‚ö†Ô∏è Configure via env var
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"

# Localiza√ß√£o do modelo
# Op√ß√£o 1: Mesmo disco do projeto (C:) - RECOMENDADO
LOCAL_MODEL_DIR = Path("./models/qwen2.5-7b-instruct").resolve()
# Resultado: C:\Users\marco\Superezio Realtime\models\qwen2.5-7b-instruct\

# Op√ß√£o 2: Disco D: (descomente se preferir)
# LOCAL_MODEL_DIR = Path("D:/models/qwen2.5-7b-instruct")
# ou
# LOCAL_MODEL_DIR = Path("D:/SuperEzio/models/qwen2.5-7b-instruct")

def download_model():
    """Baixa modelo do HF e salva localmente (UMA VEZ)"""
    print("=" * 60)
    print("üì• DOWNLOAD DE MODELO - HUGGING FACE")
    print("=" * 60)
    print(f"üéØ Modelo: {MODEL_NAME}")
    print(f"üíæ Destino: {LOCAL_MODEL_DIR}")
    print(f"üìä Tamanho estimado: ~5-7 GB")
    print()
    print("‚ö†Ô∏è  Este download acontece UMA VEZ.")
    print("‚úÖ Depois disso, o modelo funciona 100% offline!")
    print()
    
    # Criar diret√≥rio se n√£o existir
    LOCAL_MODEL_DIR.mkdir(parents=True, exist_ok=True)
    
    try:
        print("üîÑ Baixando modelo...")
        print("   (Isso pode levar 10-30 minutos dependendo da internet)")
        print()
        
        # Baixar modelo completo do Hugging Face
        snapshot_download(
            repo_id=MODEL_NAME,
            token=HF_TOKEN,
            local_dir=str(LOCAL_MODEL_DIR),
            local_dir_use_symlinks=False,  # Copiar arquivos, n√£o symlinks
        )
        
        # Verificar se baixou corretamente
        if not (LOCAL_MODEL_DIR / "config.json").exists():
            raise FileNotFoundError("Modelo n√£o foi baixado corretamente")
        
        print()
        print("=" * 60)
        print("‚úÖ DOWNLOAD CONCLU√çDO!")
        print("=" * 60)
        print(f"üìç Localiza√ß√£o: {LOCAL_MODEL_DIR}")
        print(f"üíæ Tamanho: ~{get_dir_size(LOCAL_MODEL_DIR) / 1024**3:.2f} GB")
        print()
        print("üöÄ Agora voc√™ pode usar o modelo 100% OFFLINE!")
        print("   Execute: python server/hf_inference.py")
        print()
        print("üåê Voc√™ pode desconectar a internet agora.")
        print("   O modelo n√£o precisa mais do Hugging Face!")
        
    except Exception as e:
        print()
        print("=" * 60)
        print("‚ùå ERRO NO DOWNLOAD")
        print("=" * 60)
        print(f"Erro: {e}")
        print()
        print("üí° Verifique:")
        print("   1. Token do Hugging Face est√° correto?")
        print("   2. Voc√™ tem espa√ßo em disco? (~7GB)")
        print("   3. Conex√£o com internet est√° funcionando?")
        sys.exit(1)

def get_dir_size(path: Path) -> int:
    """Calcula tamanho do diret√≥rio"""
    total = 0
    for entry in path.rglob("*"):
        if entry.is_file():
            total += entry.stat().st_size
    return total

if __name__ == "__main__":
    download_model()

