# ‚úÖ Verifica√ß√£o: Qual Modelo Est√° Sendo Usado

## üîç PROBLEMA IDENTIFICADO

O frontend estava usando **Ollama** (`sendMessageToOllama`) mesmo quando o modelo selecionado era "Qwen 2.5 7B".

**Caminho atual:**
```
Frontend ‚Üí sendMessageToOllama ‚Üí http://localhost:11434/api/chat (Ollama)
```

**Caminho correto (para Qwen local):**
```
Frontend ‚Üí sendMessageToHF ‚Üí /api/hf/chat ‚Üí http://localhost:8000/chat (Python FastAPI)
```

---

## ‚úÖ CORRE√á√ÉO APLICADA

### **1. Atualizado `src/hooks/useChat.ts`:**
- Adicionado import de `sendMessageToHF`
- Adicionada detec√ß√£o autom√°tica:
  - Se modelo cont√©m "qwen" ou "Qwen2.5" ‚Üí usa Hugging Face local
  - Caso contr√°rio ‚Üí usa Ollama

### **2. Atualizado `src/services/huggingfaceClient.ts`:**
- Adicionada mesma l√≥gica de APIs externas (clima, cripto)
- Adicionada busca web
- Adicionado RAG (contexto de mem√≥ria)

---

## üìä DETEC√á√ÉO AUTOM√ÅTICA

O sistema agora detecta automaticamente:

```typescript
const isLocalModel = modelToUse.toLowerCase().includes('qwen2.5') || 
                    modelToUse.toLowerCase().includes('qwen') ||
                    modelToUse === 'Qwen2.5-7B-Instruct';

const botResponseContent = isLocalModel
  ? await sendMessageToHF([...messages, userMessage], undefined, modelToUse)
  : await sendMessageToOllama([...messages, userMessage], modelToUse);
```

**Modelos que usam Hugging Face local:**
- ‚úÖ "Qwen 2.5 7B"
- ‚úÖ "Qwen2.5-7B-Instruct"
- ‚úÖ "qwen2.5:7b-instruct"
- ‚úÖ Qualquer modelo com "qwen" no nome

**Modelos que usam Ollama:**
- ‚úÖ "llama3:8b"
- ‚úÖ "phi3:mini"
- ‚úÖ Outros modelos do Ollama

---

## üîç COMO VERIFICAR

### **1. Verificar no Console do Navegador:**
Abra DevTools (F12) ‚Üí Console ‚Üí Procure por:
- `[AGENT]` - indica uso de agent tools
- `Error communicating with Hugging Face backend` - erro no backend Python
- `Error communicating with Ollama` - erro no Ollama

### **2. Verificar Network Tab:**
- **Hugging Face local**: Requisi√ß√µes para `/api/hf/chat`
- **Ollama**: Requisi√ß√µes para `/ollama/api/chat` ou `http://localhost:11434/api/chat`

### **3. Verificar Servidores:**
```bash
# Python FastAPI (porta 8000)
netstat -ano | findstr ":8000"

# Ollama (porta 11434)
netstat -ano | findstr ":11434"
```

---

## ‚úÖ STATUS

- [x] Detec√ß√£o autom√°tica implementada
- [x] `sendMessageToHF` atualizado com RAG e APIs externas
- [x] Compat√≠vel com ambos os backends (Ollama e Hugging Face)

**Pr√≥ximo:** Testar selecionando "Qwen 2.5 7B" no dropdown e verificar se usa o backend Python.

