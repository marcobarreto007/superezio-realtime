# üöÄ Como Usar o Backend Python (Hugging Face)

**Status:** ‚úÖ Implementa√ß√£o completa

---

## üìã PR√â-REQUISITOS

- ‚úÖ Python 3.12+ instalado
- ‚úÖ CUDA instalado (para GPU)
- ‚úÖ Modelo Qwen2.5-7B baixado (14.2 GB)
- ‚úÖ Ambiente virtual criado (`backend/venv/`)

---

## üöÄ INICIAR SERVIDOR

### **Op√ß√£o 1: Script Batch (Windows)**
```bash
cd backend
start.bat
```

### **Op√ß√£o 2: Manual**
```bash
cd backend
venv\Scripts\activate
python api.py
```

**Servidor roda em:** `http://localhost:8000`

---

## ‚úÖ VERIFICA√á√ÉO

### **1. Verificar Sa√∫de:**
```bash
curl http://localhost:8000/health
```

**Resposta esperada:**
```json
{
  "status": "healthy",
  "gpu_available": true,
  "gpu_name": "NVIDIA GeForce RTX 3060",
  "gpu_memory_total_gb": 12.0,
  "gpu_memory_used_gb": 5.5,
  "model_loaded": true
}
```

### **2. Testar Chat:**
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "system", "content": "Voc√™ √© SuperEzio."},
      {"role": "user", "content": "Oi!"}
    ],
    "temperature": 0.2,
    "max_tokens": 512
  }'
```

---

## üîó INTEGRA√á√ÉO COM EXPRESS

O Express (`server.mjs`) j√° est√° configurado para fazer proxy:

**Frontend ‚Üí Express ‚Üí FastAPI**

```
http://localhost:3000/api/hf/chat
  ‚Üí http://localhost:8080/api/hf/chat (Express)
    ‚Üí http://localhost:8000/chat (FastAPI)
```

---

## üìä ENDPOINTS DISPON√çVEIS

### **GET /** - Informa√ß√µes do servidor
```json
{
  "status": "online",
  "model": "Qwen2.5-7B-Instruct",
  "device": "cuda",
  "gpu_memory_used_gb": 5.5
}
```

### **GET /health** - Status de sa√∫de
```json
{
  "status": "healthy",
  "gpu_available": true,
  "model_loaded": true
}
```

### **POST /chat** - Chat completion
```json
{
  "messages": [
    {"role": "user", "content": "Ol√°!"}
  ],
  "temperature": 0.2,
  "max_tokens": 2048,
  "tools": [...] (opcional)
}
```

---

## üêõ TROUBLESHOOTING

### **Erro: "Modelo n√£o encontrado"**
```bash
# Verificar se modelo existe
Test-Path "models\qwen2.5-7b-instruct\config.json"

# Se n√£o existir, baixar:
python scripts/download_model.py
```

### **Erro: "CUDA n√£o dispon√≠vel"**
```bash
# Verificar CUDA
python -c "import torch; print(torch.cuda.is_available())"

# Se False, reinstalar PyTorch com CUDA:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### **Erro: "Port 8000 already in use"**
```bash
# Matar processo na porta 8000
netstat -ano | findstr :8000
taskkill /PID <PID> /F
```

---

## ‚úÖ CHECKLIST DE TESTE

- [ ] Servidor Python inicia sem erros
- [ ] Modelo carrega na GPU
- [ ] Endpoint `/health` responde
- [ ] Endpoint `/chat` funciona
- [ ] Express faz proxy corretamente
- [ ] Frontend consegue chamar `/api/hf/chat`

---

**Status:** ‚úÖ Pronto para uso  
**Pr√≥ximo:** Testar integra√ß√£o completa

